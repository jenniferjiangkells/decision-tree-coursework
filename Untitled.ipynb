{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    f1, classification = Inner_validation(training_data, test_data)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def cross_validation(dataset, Pruned_or_Raw):\n",
    "\tmodels_array = []\n",
    "\tdepth_array = []\n",
    "\n",
    "\tcmat_sum = np.zeros((4,4))\n",
    "\tprecision_sum = np.zeros((4))\n",
    "\trecall_sum = np.zeros((4))\n",
    "\tf1_sum = np.zeros((4))\n",
    "\tclass_rate_sum = 0\n",
    "\n",
    "\tpruned_cmat = np.zeros((4,4))\n",
    "\tpruned_precision = np.zeros((4))\n",
    "\tpruned_recall = np.zeros((4))\n",
    "\tpruned_f1 = np.zeros((4))\n",
    "\tpruned_class = 0\n",
    "\n",
    "\tfor i in range(10):\n",
    "\t\tactual_labels=[]\n",
    "\t\tpredicted_labels = []\n",
    "\n",
    "\t\tstart = int(len(dataset) * i / 10)\n",
    "\t\tend = int(len(dataset) * (i + 1) / 10)\n",
    "\t\ttest_data = dataset[start:end]\n",
    "\n",
    "\t\ttraining_data = dataset[:start]\n",
    "\t\ttraining_data.extend(dataset[end:])\n",
    "\n",
    "\t\t# if we are evaluating pruned model. We get an array of ten pruned models from the Inner_validation. We want to\n",
    "\t\t# get all the average stats for the 10*10 pruned models\n",
    "\t\tif (Pruned_or_Raw == 1):\n",
    "\t\t\tmodels_array, depth_array, cmat, precision, recall, \n",
    "            f1, classification = Inner_validation(training_data, test_data)\n",
    "\n",
    "\t\t\tpruned_cmat += cmat\n",
    "\t\t\tpruned_precision += precision\n",
    "\t\t\tpruned_recall += recall\n",
    "\t\t\tpruned_f1 += f1\n",
    "\t\t\tpruned_class += classification\n",
    "\n",
    "\t\t\t#print(models_array)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tmodel, depth = decision_tree_training(training_data)\n",
    "\t\t\tdepth_array.append(depth)\n",
    "\n",
    "\t\t\tmodels_array.append(model)\n",
    "\n",
    "\t\t\tfor data in test_data:\n",
    "\t\t\t\tlabel = int(data[-1])\n",
    "\t\t\t\tpredicted = predict(data, model)\n",
    "\t\t\t\tactual_labels.append(label)\n",
    "\t\t\t\tpredicted_labels.append(predicted)\n",
    "\n",
    "\t\t\tcmat, precision, recall, f1, classification = get_stats(actual_labels, predicted_labels)\n",
    "\t\t\tcmat_sum += cmat\n",
    "\t\t\tprecision_sum += precision\n",
    "\t\t\trecall_sum += recall\n",
    "\t\t\tf1_sum += f1\n",
    "\t\t\tclass_rate_sum += classification\n",
    "\n",
    "\tif (Pruned_or_Raw == 1):\n",
    "\t\tprint(\"Average confusion matrix:\\n\", pruned_cmat/10)\n",
    "\t\tprint(\"Average precision rate: \\n\", pruned_precision/10)\n",
    "\t\tprint(\"Average recall rate: \\n\", pruned_recall/10)\n",
    "\t\tprint(\"Average F1 measure: \\n\", pruned_f1/10)\n",
    "\t\tprint(\"Average classification rate: \\n\", pruned_class/10)\n",
    "\n",
    "\t\tprint(\"Depth of the ten pruned trees are: \", depth_array)\n",
    "\telse:\n",
    "\n",
    "\t\tprint(\"Average confusion matrix:\\n\", cmat_sum/10)\n",
    "\t\tprint(\"Average precision rate: \\n\", precision_sum/10)\n",
    "\t\tprint(\"Average recall rate: \\n\", recall_sum/10)\n",
    "\t\tprint(\"Average F1 measure: \\n\", f1_sum/10)\n",
    "\t\tprint(\"Average classification rate: \\n\", class_rate_sum/10)\n",
    "\n",
    "\t\tprint(\"Depth of the ten unpruned trees are: \", depth_array)\n",
    "\n",
    "\treturn models_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inner_validation(dataset, test_data):\n",
    "\tmodels_array = []\n",
    "\tdepth_array = []\n",
    "\n",
    "\tcmat_sum = np.zeros((4,4))\n",
    "\tprecision_sum = np.zeros((4))\n",
    "\trecall_sum = np.zeros((4))\n",
    "\tf1_sum = np.zeros((4))\n",
    "\tclass_rate_sum = 0\n",
    "\n",
    "\tfor i in range(10):\n",
    "\t\tactual_labels=[]\n",
    "\t\tpredicted_labels = []\n",
    "\n",
    "\t\tstart = int(len(dataset) * i / 10)\n",
    "\t\tend = int(len(dataset) * (i + 1) / 10)\n",
    "\t\tvalidation_data = dataset[start:end]\n",
    "\n",
    "\t\ttraining_data = dataset[:start]\n",
    "\t\ttraining_data.extend(dataset[end:])\n",
    "\n",
    "\t\tmodel, depth = decision_tree_training(training_data)\n",
    "\t\tdepth_array.append(depth)\n",
    "\n",
    "\t\t# we pass in the validation_data into the prune function for pruning\n",
    "\t\tpruned_model = prune(model, validation_data, model)\n",
    "\t\tmodels_array.append(pruned_model)\n",
    "\n",
    "\t\tfor data in validation_data:\n",
    "\t\t\tlabel = int(data[-1])\n",
    "\t\t\tpredicted = predict(data, pruned_model)\n",
    "\t\t\tactual_labels.append(label)\n",
    "\t\t\tpredicted_labels.append(predicted)\n",
    "\n",
    "\t\tcmat, precision, recall, f1, classification = get_stats(actual_labels, predicted_labels)\n",
    "\t\tcmat_sum += cmat\n",
    "\t\tprecision_sum += precision\n",
    "\t\trecall_sum += recall\n",
    "\t\tf1_sum += f1\n",
    "\t\tclass_rate_sum += classification\n",
    "\n",
    "\tavg_cm = cmat_sum/10\n",
    "\tavg_precision = precision_sum/10\n",
    "\tavg_recall = recall_sum/10\n",
    "\tavg_f1 = f1_sum/10\n",
    "\tavg_classification = class_rate_sum/10\n",
    "\n",
    "\n",
    "\n",
    "\treturn models_array, depth_array, avg_cm, avg_precision, avg_recall, avg_f1, avg_classification "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
